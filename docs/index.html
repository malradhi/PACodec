<!DOCTYPE html>
<html lang="eng">



  <head>
    <title>PACodec</title>
      <article>
        <header>
		

		
  
<p><br> 
</br></p>
	
	<h1>Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum</h1>
        </header>
      </article>
	  <p>  </p>
	  
	  <p><b>Authors:</b> <a href="https://malradhi.github.io/" target="_blank">Mohammed Salah Al-Radhi</a>, Riad Larbi, Mátyás Bartalis, Géza Németh</p>
	  <p> 
	  </p>  
<p> 
</p>





<p>
<b>Abstract: </b>
Neural vocoders are central to speech synthesis; despite
their success, most still suffer from limited prosody
modeling and inaccurate phase reconstruction. We propose a
vocoder that introduces prosody-guided harmonic attention
to enhance voiced-segment encoding and directly predicts
complex-spectral components for waveform synthesis via
inverse STFT. Unlike mel-spectrogram–based approaches,
our design jointly models magnitude and phase, ensuring
phase coherence and improved pitch fidelity. To further
align with perceptual quality, we adopt a multi-objective
training strategy that integrates adversarial, spectral, and
phase-aware losses. Experiments on benchmark datasets
demonstrate consistent gains over HiFi-GAN and
AutoVocoder: F0-RMSE reduced by 22%, voiced/unvoiced
error lowered by 18%, and MOS scores improved by 0.15.
These results show that prosody-guided attention combined
with direct complex-spectrum modeling yields more natural,
pitch-accurate, and robust synthetic speech, setting a strong
foundation for expressive neural vocoding.
</p>

<!-- <p align="left"><img width="40%" src="../img/model.jpg" /></p> -->

<p align="left">
  <img width="60%" src="https://raw.githubusercontent.com/malradhi/MiSTR/main/img/model.jpg" alt="Model Architecture" />
</p>




<hr>



<br />
<br />


<div>	
      <table class="table">
	<thead>
	  <tr>
	    <th>Natural</th>
	    <th>Reference Model [1]</th>
		<th>MiSTR</th>
	  </tr>
	</thead>
	<tbody>
	  <tr>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/original/sub-01_orig_audio.wav" type="audio/wav">
	      </audio>
	    </td>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/baseline/sub-01_synthesized.wav" type="audio/wav">
	      </audio>
	    </td>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/MiSTR/sub-01_predicted.wav" type="audio/wav">
	      </audio>
	    </td>		
	  </tr>
	</tbody>
      </table>
	</div>
	  
	  




<div>	
      <table class="table">
	<thead>
	  <tr>
	    <th>Natural</th>
	    <th>Reference Model [1]</th>
		<th>MiSTR</th>
	  </tr>
	</thead>
	<tbody>
	  <tr>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/original/sub-02_orig_audio.wav" type="audio/wav">
	      </audio>
	    </td>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/baseline/sub-02_synthesized.wav" type="audio/wav">
	      </audio>
	    </td>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/MiSTR/sub-02_predicted.wav" type="audio/wav">
	      </audio>
	    </td>		
	  </tr>
	</tbody>
      </table>
	</div>




<div>	
      <table class="table">
	<thead>
	  <tr>
	    <th>Natural</th>
	    <th>Reference Model [1]</th>
		<th>MiSTR</th>
	  </tr>
	</thead>
	<tbody>
	  <tr>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/original/sub-03_orig_audio.wav" type="audio/wav">
	      </audio>
	    </td>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/baseline/sub-03_synthesized.wav" type="audio/wav">
	      </audio>
	    </td>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/MiSTR/sub-03_predicted.wav" type="audio/wav">
	      </audio>
	    </td>		
	  </tr>
	</tbody>
      </table>
	</div>

  
	
	
	
	
<div>	
      <table class="table">
	<thead>
	  <tr>
	    <th>Natural</th>
	    <th>Reference Model [1]</th>
		<th>MiSTR</th>
	  </tr>
	</thead>
	<tbody>
	  <tr>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/original/sub-08_orig_audio.wav" type="audio/wav">
	      </audio>
	    </td>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/baseline/sub-08_synthesized.wav" type="audio/wav">
	      </audio>
	    </td>
	    <td>
	      <audio controls="">
		<source src="https://raw.githubusercontent.com/malradhi/MiSTR/main/samples/MiSTR/sub-08_predicted.wav" type="audio/wav">
	      </audio>
	    </td>		
	  </tr>
	</tbody>
      </table>
	</div>	
	
	


<br />
<br />	  



	
<p><b>Reference Model [1]:</b> M. Verwoert, M.C. Ottenhoff, S. Goulis, et al., "Dataset of Speech Production in Intracranial Electroencephalography," <i>Scientific Data</i>, vol. 9, pp. 1–9, 2022. <a href="https://doi.org/10.1038/s41597-022-01542-9" target="_blank">[DOI]</a></p>
	
	
<hr>	
	
	



<p align="left">
  <img width="40%" src="https://raw.githubusercontent.com/malradhi/MiSTR/main/img/compare.png" alt="Model Architecture" />
</p>



<hr>
	
  </head>
</html>
